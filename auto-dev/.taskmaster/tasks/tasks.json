{
  "master": {
    "tasks": [
      {
        "id": "11",
        "title": "Set up .autodev directory structure and rules.md initialization",
        "description": "Create and initialize the .autodev directory with a rules.md file to store learned business rules from fixes. This provides the foundation for the experience learning mechanism.",
        "details": "Create the .autodev directory in the project root if it doesn't exist. Initialize .autodev/rules.md with a header '# 项目经验规则' and ensure the file is ready to append new rules. Add logic to auto-create this directory during plugin initialization. Consider adding .autodev/rules.md to version control (or .gitignore if preferred). Implement file system checks and creation with proper error handling.",
        "testStrategy": "Verify that .autodev directory is created on plugin initialization. Confirm .autodev/rules.md exists with correct header. Test that the file can be read and written to. Verify directory structure persists across plugin restarts.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T08:10:16.144Z"
      },
      {
        "id": "12",
        "title": "Implement /auto-dev:fix command parser and context capture",
        "description": "Create the /auto-dev:fix command handler that parses user input, captures the problem description, and associates it with the current task context from the Ralph Loop execution.",
        "details": "Implement command parsing for '/auto-dev:fix \"problem description\"' format. Extract the problem description from quotes. Capture current execution context including: current task ID, task description, current file being edited, and execution state. Store this context in memory for the fix workflow. Implement error handling for malformed commands and missing context. Consider using regex for robust parsing: /^\\/auto-dev:fix\\s+\"(.+)\"$/",
        "testStrategy": "Test command parsing with various input formats (with/without quotes, special characters). Verify context capture includes task ID, description, and file path. Test error handling for malformed commands. Confirm context is available for downstream processing.",
        "priority": "high",
        "dependencies": [
          "11"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T08:10:51.833Z"
      },
      {
        "id": "13",
        "title": "Implement code modification and verification in fix workflow",
        "description": "Create the code modification logic that applies fixes based on the problem description and runs verification to confirm the fix is successful.",
        "details": "Implement a fix execution module that: (1) Takes the problem description and current code context, (2) Uses Claude API to generate a fix based on the problem, (3) Applies the fix to the relevant file, (4) Runs existing test suite or validation checks to verify the fix works. Store the before/after code diff for later analysis. Implement rollback capability if verification fails. Consider using git diff or similar for tracking changes. Handle file I/O with proper error handling and backup creation.",
        "testStrategy": "Test fix application on sample code with known issues. Verify before/after diffs are captured correctly. Test verification logic with passing and failing scenarios. Confirm rollback works when verification fails. Test with various file types and code structures.",
        "priority": "high",
        "dependencies": [
          "12"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T08:10:51.834Z"
      },
      {
        "id": "14",
        "title": "Implement learn-from-fix skill for rule extraction and analysis",
        "description": "Create the learn-from-fix skill that analyzes code differences, extracts generalizable business rules, and formats them for user confirmation.",
        "details": "Implement skill logic that: (1) Analyzes the before/after code diff from the fix, (2) Uses Claude API to extract generalizable business rules in format 'When [situation], should [action]', (3) Generates rule description with context about why the rule matters, (4) Formats rule metadata including date, source task, reason, and affected files. Ensure rules are specific enough to be actionable but general enough to be reusable. Store extracted rule candidate in memory for user confirmation step.",
        "testStrategy": "Test rule extraction with various code changes (logic changes, data transformations, UI updates). Verify rule format matches 'When X, should Y' pattern. Test that rules are generalizable and not overly specific. Confirm metadata (date, task, files) is captured correctly. Test with edge cases like multi-file changes.",
        "priority": "high",
        "dependencies": [
          "13"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T08:11:25.059Z"
      },
      {
        "id": "15",
        "title": "Implement user confirmation dialog for rule recording",
        "description": "Create an interactive dialog that presents the extracted rule to the user and captures their decision to remember or skip the rule.",
        "details": "Implement a user-facing dialog component that displays: (1) Confirmation that the fix was applied successfully, (2) The extracted rule in clear language, (3) Two action buttons: '记住' (Remember) and '这次算了' (Skip this time). Handle both button clicks and timeout scenarios. Return user's choice to the fix workflow. Consider implementing this as a prompt-based interaction compatible with the Claude interface. Store user's choice for the next step.",
        "testStrategy": "Test dialog rendering with various rule descriptions. Verify both button actions are captured correctly. Test timeout handling if user doesn't respond. Confirm dialog doesn't block other operations. Test with long rule descriptions and special characters.",
        "priority": "high",
        "dependencies": [
          "14"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T08:11:25.059Z"
      },
      {
        "id": "16",
        "title": "Implement rule persistence to .autodev/rules.md",
        "description": "Create the logic to append confirmed rules to the .autodev/rules.md file with proper formatting and metadata.",
        "details": "Implement file append logic that: (1) Takes the confirmed rule and metadata, (2) Formats it according to the spec: '## [Date] 从任务 \"[Task]\" 学到的规则' followed by rule details, (3) Appends to .autodev/rules.md with proper markdown formatting, (4) Handles file locking and concurrent writes, (5) Validates write success. Use ISO date format (YYYY-MM-DD) for consistency. Implement error handling for file system errors. Consider implementing a backup before write.",
        "testStrategy": "Test rule appending with various rule formats. Verify markdown formatting is correct. Test concurrent write scenarios. Confirm file integrity after multiple appends. Test with special characters in rule descriptions. Verify date formatting is consistent.",
        "priority": "high",
        "dependencies": [
          "15"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T08:11:25.060Z"
      },
      {
        "id": "17",
        "title": "Implement Ralph Loop continuation logic in fix command",
        "description": "Create logic to detect Ralph Loop execution state and either automatically continue the next iteration or prompt the user to restart.",
        "details": "Implement state detection that: (1) Checks if Ralph Loop is currently running (maintain execution state flag), (2) If running: automatically trigger the next iteration after fix completion, (3) If stopped: prompt user with 'Would you like to restart the Ralph Loop?' with Yes/No options. Store Ralph Loop state in execution context. Implement proper state transitions. Handle edge cases like nested fix commands during loop execution.",
        "testStrategy": "Test loop continuation when Ralph Loop is active. Verify automatic next iteration is triggered. Test user prompt when loop is stopped. Confirm state transitions are correct. Test nested fix scenarios. Verify loop doesn't restart unexpectedly.",
        "priority": "high",
        "dependencies": [
          "16"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T08:11:48.941Z"
      },
      {
        "id": "18",
        "title": "Update plugin.json with V0.2 configuration",
        "description": "Update the plugin configuration file to reflect V0.2 features including the fix command and learn-from-fix skill.",
        "details": "Update plugin.json with: (1) Version bumped to '0.2.0', (2) Description updated to '自动化开发助手 - 快速迭代，持续学习', (3) Commands array includes both 'auto-dev' and 'fix', (4) Skills array includes 'execute-loop' and 'learn-from-fix', (5) Maintain mcp_dependencies: ['taskmaster-ai']. Ensure JSON is valid and properly formatted. Consider adding changelog entry.",
        "testStrategy": "Validate plugin.json syntax. Verify all required fields are present. Confirm version is correctly updated. Test that plugin loads with new configuration. Verify commands and skills are registered correctly.",
        "priority": "medium",
        "dependencies": [
          "11"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T08:11:48.942Z"
      },
      {
        "id": "19",
        "title": "Create comprehensive integration tests for fix workflow",
        "description": "Implement end-to-end tests covering the complete fix workflow from command invocation through rule persistence.",
        "details": "Create test suite that covers: (1) Command parsing with various inputs, (2) Context capture from Ralph Loop, (3) Code modification and verification, (4) Rule extraction accuracy, (5) User confirmation dialog, (6) Rule persistence to file, (7) Loop continuation logic, (8) Error scenarios and edge cases. Use mock data for code samples and user responses. Test the complete workflow in sequence. Include tests for boundary conditions mentioned in PRD test tables.",
        "testStrategy": "Create integration test cases for each component interaction. Test happy path: fix command → code modification → rule extraction → user confirms → rule saved → loop continues. Test sad paths: verification fails, user rejects rule, loop is stopped. Verify all state transitions. Test with realistic code samples and problem descriptions.",
        "priority": "medium",
        "dependencies": [
          "17",
          "18"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T08:11:48.943Z"
      },
      {
        "id": "20",
        "title": "Implement error handling and recovery mechanisms",
        "description": "Create robust error handling throughout the fix workflow to gracefully handle failures and provide clear feedback to users.",
        "details": "Implement error handling for: (1) File system errors (read/write failures), (2) Code modification failures, (3) Verification failures, (4) API errors during rule extraction, (5) Invalid user input, (6) State inconsistencies. For each error, provide: clear error message, suggested recovery action, and rollback capability where applicable. Log errors for debugging. Implement retry logic for transient failures. Ensure user can always exit the workflow gracefully.",
        "testStrategy": "Test each error scenario individually. Verify error messages are clear and actionable. Confirm rollback works correctly. Test retry logic with simulated transient failures. Verify logging captures sufficient context. Test user can exit at any point.",
        "priority": "medium",
        "dependencies": [
          "17"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T08:11:48.944Z"
      },
      {
        "id": "21",
        "title": "Create documentation and usage examples for fix command",
        "description": "Write comprehensive documentation for the /auto-dev:fix command including usage examples, best practices, and troubleshooting guide.",
        "details": "Create documentation covering: (1) Command syntax and parameters, (2) Multiple usage examples (login redirect, price calculation, etc.), (3) How the rule learning mechanism works, (4) Best practices for problem descriptions, (5) How to view and manage learned rules in .autodev/rules.md, (6) Troubleshooting common issues, (7) Integration with Ralph Loop. Include screenshots or diagrams if applicable. Update main README to reference this documentation.",
        "testStrategy": "Review documentation for clarity and completeness. Verify all examples are accurate and runnable. Test that documentation examples work as described. Confirm troubleshooting guide addresses common issues. Get feedback from test users.",
        "priority": "low",
        "dependencies": [
          "19"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T08:11:48.945Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2026-01-13T08:11:48.945Z",
      "taskCount": 11,
      "completedCount": 11,
      "tags": [
        "master"
      ]
    }
  }
}